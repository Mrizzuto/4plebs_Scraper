{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4plebs Archive Scraper\n",
    "\n",
    "Max Rizzuto | DFRLAB 2022\n",
    "\n",
    "------\n",
    "This notebook uses Selenium and Firefox's Gecko Driver to retreave the results of a structured query on the 4chan archive [archive.4plebs.org](archive.4plebs.org).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step-by-Step of the building process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "This scraper will be built in following order:\n",
    "* Loading the page / inputting the queries\n",
    "* Iterating through pages - I like to get this done first.\n",
    "* Identifying the list of post elements\n",
    "* Identify the elements within each post\n",
    "* Iterating through the list of page elements\n",
    "    * Creating a dictionary output for each page\n",
    "    * Adding each dictionary to either a list or a central dictionary\n",
    "* A function that will call all of the above functions, a loop to drive the whole thing.  \n",
    "* A function that exports the results as a csv\n",
    "\n",
    "Because there are so many results for our search terms of interest this scraper has got to run totally unattended. This means acknowledging when the all the results from one search have been collected and moving onto the next automatically. \n",
    "\n",
    "__Observations:__\n",
    "\n",
    "_Loading the page_ ---------------------------------------------------------------------------------------------->\n",
    "\n",
    "Boolean search does not appear to work on 4plebs. This isnt the end of the world, just means its going to take longer to run.\n",
    "Because we will likely pull the same posts for various search terms, we're going to add the search term to the post results dictionary as metadata for us to de-dup later.\n",
    "\n",
    "Also, Rather than using the search bar to search for results, we're going to use the url itself (which is sort of structured like an API). \n",
    "> ex: [archive.4plebs.org/{board}/search/text/{query}/](_)\n",
    "\n",
    "Im going to be developing the scraper on a search that only produces a few results, in this case its the search term \"reddit rules\" which returns 80 results.\n",
    "\n",
    "Something else that occurs to me is that we need to manage the list of search terms. Feeding each term to the \"get_site\" function once no more results remain.\n",
    "\n",
    "Lmao, I just ran the scraper for the first time and it detected the robotic user agent and block the request.\n",
    "They say that crawling like this is not necessary because everything is backed up on archive.org. This is only partly true and I will continue to work around this.\n",
    "They caught us because the geckdriver announces itself in the brouser's user-agent. We can generate a fake user agent to appear more authentic.\n",
    "\n",
    "This worked. I found some code [here](https://stackoverflow.com/questions/29916054/change-user-agent-for-selenium-web-driver), dropped it in and it worked great. \n",
    "\n",
    "But now I am on edge. If they put this crawling measure in place there may be others. We need to write the scraper to iteratively save progress so that if we are caught later down the line we dont lose all of our progress. \n",
    "\n",
    "_Iterating through pages_ ---------------------------------------------------------------------------------------->\n",
    "\n",
    "At the bottom of the page I see this very nice, very static \"next\" button for advancing to the next page. We're going to use that because its not going to move a whole lot.\n",
    "The list element that the next button is a child to also includes information about its status; When there are no more pages to advance to the li changes from \"next\" to \"next disabled\". We're going to use that to tell us when we've reached the last page. \n",
    "\n",
    "Selenium's ```find_elements_by_{something}``` is really interesting and useful. When using ```find_element_by_{something}``` (with ```element``` singular) if will return the first matching element. When using ```elements``` plural it will return a list of matches. \n",
    "\n",
    "We Figured out sytax for ```get_attribute(\"class\")```. It's frustrating to forget the difference between ```find_element``` and ```get_attribute```. \n",
    "\n",
    "I encountered an odd error where the next button could not be clicked itself, but the child element could. You can see in the next button function the variable ```clickable_next_button``` which uses find_element_by_xpath to advance to the child element, at which point I can click.  \n",
    "\n",
    "_Identifying the list of posts_ --------------------------------------------------------------------------------->\n",
    "\n",
    "This step has been really easy. The posts are stored in an ```<aside>``` block with the unique class \"posts\", so we can just find it using ```find_element_by_class_name(\"posts\")```. Note that this is the outer element, we want the inner list, so we do ```find_elements_by_xpath(\"./*\")``` to navagate to the various children elements. \n",
    "\n",
    "_Identifying the specific elements from each post that we want to capture_ -------------------------------------->\n",
    "I stepped through one of the items in the posts lists and pulled out each descrete value of interest. \n",
    "I've included the original process here. This was done first to make the second iteration of this process easier:\n",
    "```\n",
    "posts[0].find_element_by_tag_name(\"header\").text\n",
    "posts[0].get_attribute('id')\n",
    "post_title = posts[0].find_element_by_tag_name('h2').text\n",
    "post_author = posts[1].find_element_by_class_name(\"post_author\").text\n",
    "post_tripcode = posts[1].find_element_by_class_name(\"post_tripcode\").text\n",
    "post_hash = posts[1].find_element_by_class_name(\"poster_hash\").text\n",
    "post_datetime = posts[1].find_element_by_tag_name(\"time\").get_attribute(\"datetime\")\n",
    "post_type = posts[1].find_element_by_class_name(\"post_type\").find_element_by_xpath(\"./*\").get_attribute(\"href\")\n",
    "post_replies = \n",
    "post_text = posts[4].find_element_by_class_name(\"text\").text\n",
    "```\n",
    "\n",
    "_Iterating through the list of page elements\n",
    "    * Creating a dictionary output for each page\n",
    "    * Adding each dictionary to either a list or a central dictionary_ ----------------------------------------->\n",
    "This turned out to be extremely straight forward. You can see the function ```gather_page_results()``` and how it compared to the above code block where I hashed out each of the values we wanted to extract.\n",
    "\n",
    "I changed the variables into dictionary entries to save memory. you can declare your variables directly into the dictionary, no problem. \n",
    "\n",
    "I decided to go for the list of dictionaries because it plays extremely well with pandas, and I already know how it works. You can take the output of that fuction and put it straight into a dataframe, like so: ```pd.DataFrame(gather_page_results())```.\n",
    "\n",
    "UPDATE: its now Monday the 11th, and with a fresh pair of eyes I can see that this isnt 100% going to work. We need make the code adaptable enough to work even when elements do not exist. This means that I am going to have to use the plural ```find_elements_by_x``` approach mentioned earlier because they will return empty lists rather than error values in the event that a desired value does not exist. This empty list output, coupled with some list comprehension approach is really going save us a lot of heartache. This will effect the output, each value accessed this way will be nested in a list element of unknown length. Its my hope that the lists only ever contain one item, but its likely that there will be variation. This will create some work for us later in exchange for saving us time now. \n",
    "\n",
    "UPDATE: its now Wednesday the 13th and I've discovered that I incorrectly called ```posts``` where I should have called ```post```. This was a simple but fatal mistake and a consequence of not testing / reviewing results thoroughly enough. I have fixed the issues and added new values to the dictionary results. This will require me to re-run the code and capture a new set of data, so I cleared the cache and output folders.\n",
    "\n",
    "_A function that will call all of the above functions, a loop to drive the whole thing._ ----------------------->\n",
    "I was preparing for the worst here. After I ran the scraper a couple of times and encountered rate limiting I decided to build in some redundancies to allow us to save progress in the event that we continued to encounter errors.\n",
    "A consiquence of this is some added complexity in this otherwise pretty simple function. \n",
    "You will see the variable ```start_at_page``` is doing some strange things. It is declared in the _Input Parameters_ area below and is designed to allow the crawler to pick up where it left off after a crash inorder to avoide duplicate work. In this function it is called once (and set to be global) at which point it is set to zero, this is so that on the first run it will pick up at _x_ page of archive results, but will start the next search term at zero. In the event that the ```start_at_page``` variable is used users would need to truncate the list of terms to correspond to the values they have not yet captured. \n",
    "\n",
    "Down to brass tax---\n",
    "We've got two vital loops here: \n",
    ">An outer loop that iterates though the list of search terms we plan to query. <br>\n",
    ">It runs the function to gather page results and adds the results to a ```data``` variable and innitiates the inner loop...\n",
    "\n",
    ">The inner loop is a while loop that continues to run the ```advance_to_next_page``` function and add gathered results to the ```data``` variable. It also creates caches every 5 pages for redundancy.<br>\n",
    ">The while loop is broken when the next page button is no longer available and the ```advance_to_next_page``` function returns false. At which point the window is abandoned and a new window is opened for the next term.  \n",
    "\n",
    "_A function that exports the results as a csv_--------------------------------------------------------------------->\n",
    "Im not going to dwell on this too much but the last bit of the process is done pretty much entirely in pandas (python data science library) and im trying some new organization below which im not 100% sold on. \n",
    "Basically the section titled \"_Exporting Processed Data_\" adds a couple new columns to the data, cleans up some loose ends, and ships the data as a csv in a form that is more polished than the pickle outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Parameters\n",
    "The two cells below can be used interchangably.<br>\n",
    "For queries on just one set of terms + time spans use the first of the two below cells.<br>\n",
    "For queries on multiple terms over multiple time spans, possibly across multiple boards, use the second of the two cells and write out dictoinary elements for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_at_page = 0\n",
    "board = \"pol\"\n",
    "search_terms = [\"Tarrant\"] # Must be list \n",
    "start_date = \"2019-03-14\"\n",
    "end_date = \"2022-07-14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_at_page = 0\n",
    "queries = {\n",
    "           3:  {\"board\": \"pol\",\n",
    "                \"terms\": [\"Roof\"],\n",
    "                \"start_date\": \"2015-06-17\",\n",
    "                \"end_date\": \"2017-08-09\"\n",
    "               },\n",
    "#            4:  {\"board\": \"pol\",\n",
    "#                 \"terms\": [\"Breivik\", \"Anders\"],\n",
    "#                 \"start_date\": \"2011-07-22\",\n",
    "#                 \"end_date\": \"2022-07-14\"\n",
    "#                }\n",
    "}\n",
    "#            0: {\"board\": \"pol\",\n",
    "#                 \"terms\": [\"Tarrant\"],\n",
    "#                 \"start_date\": \"2019-03-14\",\n",
    "#                 \"end_date\": \"2022-07-14\"\n",
    "#                },\n",
    "#            1:  {\"board\": \"pol\",\n",
    "#                 \"terms\": [\"Earnest\", \"Poway\"],\n",
    "#                 \"start_date\": \"2019-04-27\",\n",
    "#                 \"end_date\": \"2022-07-14\"\n",
    "#                },\n",
    "#            2:  {\"board\": \"pol\",\n",
    "#                 \"terms\": [\"El Paso\", \"Crusius\"],\n",
    "#                 \"start_date\": \"2019-08-03\",\n",
    "#                 \"end_date\": \"2022-07-14\"\n",
    "#                },\n",
    "#            3:  {\"board\": \"pol\",\n",
    "#                 \"terms\": [\"Charleston\", \"Roof\", \"Dylann\"],\n",
    "#                 \"start_date\": \"2015-06-17\",\n",
    "#                 \"end_date\": \"2022-07-14\"\n",
    "#                },\n",
    "#            4:  {\"board\": \"pol\",\n",
    "#                 \"terms\": [\"Ut√∏ya\", \"Utoya\", \"Breivik\", \"Anders\"],\n",
    "#                 \"start_date\": \"2011-07-22\",\n",
    "#                 \"end_date\": \"2022-07-14\"\n",
    "#                }\n",
    "#          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site(page, board, search_term, start_date, end_date):\n",
    "    \"\"\" Takes query parameters as input (the board of interest, the search term of interest, the start and end dates). \n",
    "        Loads archive.4plebs.org with a random user agent to get around anti-crawling measures. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate User Agent\n",
    "    ua = UserAgent()\n",
    "    user_agent = ua.random\n",
    "\n",
    "    # Set up webdriver with user agent... \n",
    "    profile = webdriver.FirefoxProfile()\n",
    "    profile.set_preference(\"general.useragent.override\", user_agent)\n",
    "    wd = webdriver.Firefox(profile)\n",
    "    \n",
    "    # Make URL with parameters:\n",
    "    query_url = f\"http://archive.4plebs.org/{board}/search/text/{search_term}/start/{start_date}/end/{end_date}/page/{page}/\"\n",
    "    \n",
    "    # Get website with parameters. \n",
    "    try:\n",
    "        wd.get(query_url)\n",
    "        sleep(2)\n",
    "        print(\"Page Loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "                \n",
    "    return wd\n",
    "\n",
    "\n",
    "def advance_to_next_page(wd):\n",
    "    \"\"\" Does not take any inputs.\n",
    "        Advances to the next page and returns True when possible. When the \"next\" button is inactive it returns False.  \n",
    "    \"\"\"  \n",
    "    next_button = wd.find_element_by_class_name(\"next\")\n",
    "    button_status = next_button.get_attribute(\"class\")\n",
    "    clickable_next_button = next_button.find_element_by_xpath(\"./*\")\n",
    "    \n",
    "    if button_status == \"next\":\n",
    "        # Return something that will continue the loop.    \n",
    "        next_button.location_once_scrolled_into_view\n",
    "        clickable_next_button.click()\n",
    "        return True\n",
    "                #advance_to_next_page()\n",
    "    else: \n",
    "        # Return something that will kill the \"next page\" loop and will move onto the next search term.\n",
    "        print(\"Captured all results for this term, advancing to next term\")\n",
    "        return False\n",
    "    \n",
    "\n",
    "def gather_page_results(wd, term):\n",
    "    \"\"\" Takes most recent webdriver state and the search term used for embedding in page results data as input.\n",
    "        Returns a list of dictionary elements as output containing parsed page results.\n",
    "    \"\"\"\n",
    "    # A list of all the post elements on the current page\n",
    "    posts = wd.find_element_by_class_name(\"posts\").find_elements_by_xpath(\"./*\")\n",
    "    \n",
    "    # A list to store all the post data once its been parsed\n",
    "    post_list = []\n",
    "    \n",
    "    # Post Parsing Loop - makes dictionary keys and values for each post element. \n",
    "    for post in posts:   \n",
    "        \n",
    "        post_values = {\"search_term\": term,\n",
    "                       \"current_url\": wd.current_url,\n",
    "                       \"post_id\": post.get_attribute('id'),\n",
    "                       \"post_archive_url\": [v.get_attribute(\"href\") for v in post.find_elements_by_link_text(\"No.\")],\n",
    "                       \"post_file\": [v.text for v in post.find_elements_by_class_name(\"post_file_filename\")],\n",
    "                       \"post_title\": [v.text for v in post.find_elements_by_tag_name(\"h2\")],\n",
    "                       \"post_author\": [v.text for v in post.find_elements_by_class_name(\"post_author\")],\n",
    "                       \"post_tripcode\": [v.text for v in post.find_elements_by_class_name(\"post_tripcode\")],\n",
    "                       \"post_hash\": [v.text for v in post.find_elements_by_class_name(\"poster_hash\")],\n",
    "                       \"post_datetime\": post.find_element_by_tag_name(\"time\").get_attribute(\"datetime\"),\n",
    "                       \"post_header\": [v.text for v in post.find_element_by_class_name(\"post_type\").find_elements_by_xpath(\"..\")],\n",
    "                       \"post_type\": [v.get_attribute('href') for v in post.find_element_by_class_name(\"post_type\").find_elements_by_xpath(\"./*\")],\n",
    "                       \"post_text\": [v.text for v in post.find_elements_by_class_name(\"text\")]\n",
    "                      }\n",
    "        \n",
    "        post_list.append(post_values)\n",
    "    \n",
    "    return post_list\n",
    "\n",
    "\n",
    "def run_code(board, search_terms, start_date, end_date):\n",
    "    \"\"\" Takes parameters to run other functions as input.\n",
    "        Makes periodic caches of pkl files in the results folder.\n",
    "        Returns a list of dictionary elements for each post from each page of results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # make start at page a global variable\n",
    "    global start_at_page\n",
    "    \n",
    "    # a counter for saving a cache of progress\n",
    "    i = 0\n",
    "    \n",
    "    # a list to contain data from every page of results\n",
    "    data = []\n",
    "    \n",
    "    # loop for each term\n",
    "    for term in search_terms:\n",
    "        # Determining the page to start on if crawl was interupted.\n",
    "        # Although you need to manually pair down the list of assets to scrape,\n",
    "        # this start_at_page variable will set itself to zero after one execution, \n",
    "        # enusing future pages start where they should.  \n",
    "        \n",
    "        if start_at_page != 0:\n",
    "            wd = get_site(start_at_page, board, term, start_date, end_date)\n",
    "            start_at_page = 0\n",
    "        else: \n",
    "            wd = get_site(0, board, term, start_date, end_date)\n",
    "                \n",
    "        sleep(5)\n",
    "        \n",
    "        data += gather_page_results(wd, term)\n",
    "        i += 1\n",
    "        # while the next button is operable, click it and run the loop below.\n",
    "        while advance_to_next_page(wd) == True:\n",
    "            sleep(10)\n",
    "            # See if the page gave us a search limit exceeded error.\n",
    "            # If so, wait 20 seconds, refresh the page, try to gather results again. \n",
    "            try:\n",
    "                data += gather_page_results(wd, term)\n",
    "                i += 1\n",
    "                if i % 5 == 0:\n",
    "                    print(f\"Caching: {i}\", end=\"\\r\")\n",
    "                    pd.DataFrame(data).to_pickle(f\"cache/cache.pkl\")\n",
    "            \n",
    "            except:    \n",
    "                print(wd.find_element_by_class_name(\"alert\").text)\n",
    "                sleep(20)\n",
    "                wd.refresh\n",
    "                \n",
    "                data += gather_page_results(wd, term)\n",
    "                i += 1\n",
    "                if i % 5 == 0:\n",
    "                    print(f\"Caching: {i}\", end=\"\\r\")\n",
    "                    pd.DataFrame(data).to_pickle(f\"cache/cache.pkl\")\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "        pd.DataFrame(data).to_pickle(f\"results/term_{term}.pkl\")\n",
    "        \n",
    "    pd.DataFrame(data).to_pickle(f\"results/output.pkl\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Program\n",
    "Below are code blocks for two different ways to run the code. <br>\n",
    "If you intend to use just one set of terms, dates, and boards (as specified in the first of the two __Input Parameter__ blocks) then use the first of the two cells below.<br>\n",
    "If you intend to run a list of number of terms, over a number of time spans, and a number of boards, use the second and third of the two cells below; the cell containing a function called ```dictoinary_input_run_code```. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WAIT__, before running the code you need to make some folders in this notebook's working directory.\n",
    "* One folder named: \"results\"\n",
    "* another named: \"cache\"\n",
    "\n",
    "Once that is done you can proceed.\n",
    "\n",
    "A cache file will be made in the ```cache``` folder that will be updated periodically throughout the scraping process.\n",
    "<br>Once finished the an output file will be made in the ```results``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = run_code(board, search_terms, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the code above, then proceed to the _Exporting Processed Data_ section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_input_run_code(queries):\n",
    "    \"\"\" Takes parameters to run other functions as input.\n",
    "        Makes periodic caches of pkl files in the results folder.\n",
    "        Returns a list of dictionary elements for each post from each page of results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # make start at page a global variable\n",
    "    global start_at_page\n",
    "    \n",
    "    # a counter for saving a cache of progress\n",
    "    i = 0\n",
    "    \n",
    "    # a list to contain data from every page of results\n",
    "    data = []\n",
    "    \n",
    "    for k, v in queries.items():\n",
    "        #board, search_terms, start_date, end_date\n",
    "        board = v['board']\n",
    "        search_terms = v['terms']\n",
    "        start_date = v['start_date']\n",
    "        end_date = v['end_date']\n",
    "        \n",
    "        \n",
    "        # loop for each term\n",
    "        for term in search_terms:\n",
    "            # Determining the page to start on if crawl was interupted.\n",
    "            # Although you need to manually pair down the list of assets to scrape,\n",
    "            # this start_at_page variable will set itself to zero after one execution, \n",
    "            # enusing future pages start where they should.  \n",
    "\n",
    "            if start_at_page != 0:\n",
    "                wd = get_site(start_at_page, board, term, start_date, end_date)\n",
    "                start_at_page = 0\n",
    "            else: \n",
    "                wd = get_site(0, board, term, start_date, end_date)\n",
    "\n",
    "            sleep(5)\n",
    "\n",
    "            data += gather_page_results(wd, term)\n",
    "            i += 1\n",
    "            # while the next button is operable, click it and run the loop below.\n",
    "            while advance_to_next_page(wd) == True:\n",
    "                sleep(10)\n",
    "                # See if the page gave us a search limit exceeded error.\n",
    "                # If so, wait 20 seconds, refresh the page, try to gather results again. \n",
    "                try:\n",
    "                    data += gather_page_results(wd, term)\n",
    "                    i += 1\n",
    "                    if i % 5 == 0:\n",
    "                        print(f\"Caching: {i}\", end=\"\\r\")\n",
    "                        pd.DataFrame(data).to_pickle(f\"cache/cache.pkl\")\n",
    "\n",
    "                except:    \n",
    "                    print(wd.find_element_by_class_name(\"alert\").text)\n",
    "                    sleep(20)\n",
    "                    wd.refresh\n",
    "\n",
    "                    data += gather_page_results(wd, term)\n",
    "                    i += 1\n",
    "                    if i % 5 == 0:\n",
    "                        print(f\"Caching: {i}\", end=\"\\r\")\n",
    "                        pd.DataFrame(data).to_pickle(f\"cache/cache.pkl\")\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "            pd.DataFrame(data).to_pickle(f\"results/term_{term}.pkl\")\n",
    "\n",
    "    pd.DataFrame(data).to_pickle(f\"results/output.pkl\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Loaded\n",
      "Captured all results for this term, advancing to next term\n"
     ]
    }
   ],
   "source": [
    "out = dictionary_input_run_code(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Processed Data:\n",
    "Using Pandas to turn the output into a nice looking csv for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def explode_list_columns(df):\n",
    "    print(\"Exploding list cols...\")\n",
    "    return(df.explode(['post_title', 'post_author', 'post_tripcode', 'post_header','post_text', 'post_archive_url'])\n",
    "             .explode(['post_hash'])\n",
    "             .explode(['post_file'])\n",
    "             .fillna(\"\")\n",
    "          )\n",
    "\n",
    "def manage_flags(df):\n",
    "    print(\"Extracting flag info...\")\n",
    "    return(df.assign(flag_lst = lambda x: x[\"post_type\"].fillna(\"[]\").apply(lambda y: \"\" if y == [] else y[0]),\n",
    "                     post_flag = lambda x: x[\"flag_lst\"].str.split(\"/\").fillna(\"[]\").apply(lambda y: \"\" if (len(y) == 1) else y[-2]))\n",
    "              .drop([\"post_type\", \"flag_lst\"], axis=\"columns\")\n",
    "          )\n",
    "\n",
    "def get_thread_id(df):\n",
    "    print(\"Getting thread ID...\")\n",
    "    return(df.assign(thread_id = lambda x: x['post_archive_url'].str.split(\"/\").apply(lambda y: y[-2])))\n",
    "\n",
    "def manage_thread_replies(df):\n",
    "    print(\"Counting thread replies...\")\n",
    "    return(df.assign(thread_replies = lambda x: x['post_header'].astype(str).str.extract(r\"(?<=Replies\\: )(\\d+)\"))\n",
    "              .drop(\"post_header\", axis='columns')\n",
    "              .fillna(\"\")\n",
    "          )\n",
    "\n",
    "def extract_text_info(df):\n",
    "    print(\"Extracting text info...\")\n",
    "    extracted_reply_no  = df.assign(replying_to = lambda x: x['post_text'].str.findall(r\"(?<=\\>\\>)(\\d+)\").apply(set).apply(list))\n",
    "\n",
    "    unique_mentions_set = (extracted_reply_no.drop_duplicates(subset='post_id', keep='last')\n",
    "                               .explode('replying_to')[['post_id', 'replying_to']].groupby('replying_to').count().sort_values(by='post_id')\n",
    "                          )\n",
    "    _df = pd.merge(extracted_reply_no, unique_mentions_set,\n",
    "             how = 'left',\n",
    "             left_on = 'post_id',\n",
    "             right_on = 'replying_to'\n",
    "            ).rename({'post_id_x':'post_id', 'post_id_y':'replies_in_crawl'}, axis='columns')\n",
    "    \n",
    "    return(_df)\n",
    "\n",
    "def account_for_dates(df):\n",
    "    print(\"Adjusting dates to UTC...\")\n",
    "    return (df.assign(dt_split = lambda x: x['post_datetime'].str.split(\"-\"),\n",
    "                     dt_hour = lambda x: x['dt_split'].apply(lambda y: pd.Timestamp(y[-1]).hour),\n",
    "                     post_datetime_utc = lambda x: x[['dt_split', 'dt_hour']].apply(lambda y: pd.to_datetime(\"-\".join(y['dt_split'][:-1]))-pd.to_timedelta(y['dt_hour'], unit='h'), axis=1)\n",
    "                    )\n",
    "               .drop(['dt_split','dt_hour'], axis=1)\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = (df.reset_index(drop=True)\n",
    "    .pipe(explode_list_columns)\n",
    "    .pipe(manage_flags)\n",
    "    .pipe(get_thread_id)\n",
    "    .pipe(manage_thread_replies)\n",
    "    .pipe(extract_text_info)\n",
    "    .pipe(account_for_dates)\n",
    "     [[\"search_term\", \"thread_id\", \"post_id\", \"post_datetime\", \"post_datetime_utc\", \"post_text\", \"post_file\", \"post_title\", \"post_author\", \"post_tripcode\", \"post_hash\", \"post_flag\", \"thread_replies\", \"replies_in_crawl\", \"replying_to\", \"current_url\", \"post_archive_url\"]] \n",
    ")\n",
    "\n",
    "_df.to_csv('results/output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
